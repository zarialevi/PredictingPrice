{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import data_cleaning as dc\n",
    "import models as rm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and cleaning data\n",
    "First we read in the data before we have to remove the variables which aren't of interest and rename our column titles for clarity. This can be performed with pre-defined functions we have stored in a separate .py file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/products_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our models\n",
    "Here we will fit a model to our different features. We will see what value for R^2 we get and try different models in order to minimise our error and maximise our R^2.\n",
    "\n",
    "\n",
    "### Simple Linear Model\n",
    "This fits a simple linear regression model to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our initial model, our values are:\n",
      "Training r^2: 0.2524447244891891\n",
      "Training MSE: 25487.913983240684\n",
      "And for our testing dataset, our values are:\n",
      "Training r^2: 0.2574298688199398\n",
      "Training MSE: 24360.405698878036\n"
     ]
    }
   ],
   "source": [
    "X, Y, X_test, y_test, Model, Test_Model= rm.cont_model(df,'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "This method shrinks the parameters and therefore reduces the effect of multicolinearity. By doing this we also retain complexity of the model. For our data, as we don't have hundreds of parameters, this should be a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our improved model, are values are:\n",
      "Training r^2: 0.25203596278888885\n",
      "Training MSE: 25501.850722634004\n"
     ]
    }
   ],
   "source": [
    "r_model = rm.RidgeModel(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lasso Regression\n",
    "LASSO Regression (Least Absolute Shrinkage Selector Operator) -  This method reduces the values of the smaller coefficients and by doing so, selects the most important features in the model. However, if we have correlated variables it will reduce one of these variables to zero reducing the complexity of the model. This means information can be lossed. Our model doesn't have too many features however so this may not be the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our Lasso model, are values are:\n",
      "Training MSE: 26228.171674790072\n",
      "Training r^2: 0.23073311863085677\n"
     ]
    }
   ],
   "source": [
    "l_model = rm.LassoModel(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Elastic Regression\n",
    "This method effectively combines Ridge and Lasso. It groups correlated variables and finds if one of them is a strong predictor. In this case, it keeps the entire group of variables, reducing information loss. Again, we don't have high levels of multicolinearity so this may not be the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For our Elastic model, are values are:\n",
      "Training MSE: 25491.85065668974\n",
      "Training r^2: 0.25232926266650946\n"
     ]
    }
   ],
   "source": [
    "e_model = rm.ElasticModel(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cross-Validation\n",
    "To account for the random sampling of the training dataset, we apply cross-validation to it. For each fold in the dataset, this applies the model to kâ€“1 folds of the dataset. Then it tests the model on the kth fold. It repeats this for each k-fold in the dataset. We average our result to find the average error and this tells us how our model has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cross-validating the data for 10-folds: \n",
      "The average Mean Squared Errors: np.mean(-25505.965086859676) \n",
      "                 The average R2: 0.2523161523869442\n",
      "\n",
      "After cross-validating the data for 10-folds: \n",
      "The average Mean Squared Errors: np.mean(-24421.25310144052) \n",
      "                 The average R2: 0.25612365637518253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "rm.cross_validate(Model,k,X,Y)\n",
    "rm.cross_validate(Test_Model, k, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
